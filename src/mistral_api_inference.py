#!/usr/bin/env python3
"""
Script d'inf√©rence et comparaison de mod√®les via l'API Mistral

Ce script permet de:
1. Comparer les r√©ponses du mod√®le de base vs mod√®le fine-tun√©
2. Tester plusieurs prompts et afficher les r√©sultats c√¥te √† c√¥te
3. Calculer des m√©triques de comparaison (similarit√©, longueur, etc.)

Usage:
    python src/mistral_api_inference.py --base_model open-mistral-7b --fine_tuned_model ft:open-mistral-7b:XXX

Exemple:
    python src/mistral_api_inference.py \
        --base_model open-mistral-7b \
        --fine_tuned_model ft:open-mistral-7b:XXX:20240430:XXX \
        --prompts "Qu'est-ce que le PTO?" "Define KPI in one sentence."
"""

import argparse
import os
import json
from typing import List, Dict, Any, Optional
from pathlib import Path

from mistralai import Mistral
from mistralai.models import ChatCompletionResponse


def generate_response(
    client: Mistral,
    model: str,
    prompt: str,
    temperature: float = 0.7,
    max_tokens: int = 512,
) -> Dict[str, Any]:
    """
    G√©n√®re une r√©ponse avec un mod√®le via l'API Mistral.
    
    Args:
        client: Client Mistral initialis√©
        model: Nom du mod√®le √† utiliser
        prompt: Prompt √† envoyer
        temperature: Temp√©rature pour la g√©n√©ration
        max_tokens: Nombre maximum de tokens √† g√©n√©rer
        
    Returns:
        Dictionnaire avec la r√©ponse et les m√©triques
    """
    try:
        response = client.chat.complete(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=temperature,
            max_tokens=max_tokens,
        )
        
        content = response.choices[0].message.content
        usage = response.usage
        
        return {
            "content": content,
            "prompt_tokens": usage.prompt_tokens if usage else 0,
            "completion_tokens": usage.completion_tokens if usage else 0,
            "total_tokens": usage.total_tokens if usage else 0,
            "error": None,
        }
    except Exception as e:
        return {
            "content": None,
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0,
            "error": str(e),
        }


def compare_responses(
    client: Mistral,
    base_model: str,
    fine_tuned_model: str,
    prompts: List[str],
    temperature: float = 0.7,
    max_tokens: int = 512,
) -> List[Dict[str, Any]]:
    """
    Compare les r√©ponses de deux mod√®les sur une liste de prompts.
    
    Args:
        client: Client Mistral initialis√©
        base_model: Nom du mod√®le de base
        fine_tuned_model: Nom du mod√®le fine-tun√©
        prompts: Liste des prompts √† tester
        temperature: Temp√©rature pour la g√©n√©ration
        max_tokens: Nombre maximum de tokens √† g√©n√©rer
        
    Returns:
        Liste de dictionnaires avec les comparaisons
    """
    results = []
    
    for i, prompt in enumerate(prompts, 1):
        print(f"\n[{i}/{len(prompts)}] Test du prompt: {prompt[:60]}...")
        
        # G√©n√©rer avec le mod√®le de base
        print("  G√©n√©ration avec le mod√®le de base...")
        base_response = generate_response(
            client, base_model, prompt, temperature, max_tokens
        )
        
        # G√©n√©rer avec le mod√®le fine-tun√©
        print("  G√©n√©ration avec le mod√®le fine-tun√©...")
        ft_response = generate_response(
            client, fine_tuned_model, prompt, temperature, max_tokens
        )
        
        # Calculer des m√©triques de comparaison
        base_len = len(base_response["content"]) if base_response["content"] else 0
        ft_len = len(ft_response["content"]) if ft_response["content"] else 0
        
        results.append({
            "prompt": prompt,
            "base_model": base_model,
            "fine_tuned_model": fine_tuned_model,
            "base_response": base_response["content"],
            "ft_response": ft_response["content"],
            "base_error": base_response["error"],
            "ft_error": ft_response["error"],
            "base_tokens": base_response["total_tokens"],
            "ft_tokens": ft_response["total_tokens"],
            "base_length": base_len,
            "ft_length": ft_len,
            "length_diff": ft_len - base_len,
        })
        
        print(f"  ‚úì Comparaison termin√©e")
    
    return results


def print_comparison(results: List[Dict[str, Any]], detailed: bool = False):
    """
    Affiche les r√©sultats de comparaison de mani√®re format√©e.
    
    Args:
        results: Liste des r√©sultats de comparaison
        detailed: Si True, affiche les d√©tails complets
    """
    print("\n" + "="*80)
    print("R√âSULTATS DE COMPARAISON")
    print("="*80)
    
    for i, result in enumerate(results, 1):
        print(f"\n{'‚îÄ'*80}")
        print(f"Prompt {i}: {result['prompt']}")
        print(f"{'‚îÄ'*80}")
        
        if result["base_error"]:
            print(f"\n‚úó Mod√®le de base - Erreur: {result['base_error']}")
        else:
            print(f"\nüìå Mod√®le de base ({result['base_model']}):")
            print(f"   Tokens: {result['base_tokens']} | Longueur: {result['base_length']} caract√®res")
            if detailed:
                print(f"   R√©ponse: {result['base_response']}")
            else:
                preview = result['base_response'][:200] if result['base_response'] else "N/A"
                print(f"   R√©ponse: {preview}{'...' if len(result['base_response'] or '') > 200 else ''}")
        
        if result["ft_error"]:
            print(f"\n‚úó Mod√®le fine-tun√© - Erreur: {result['ft_error']}")
        else:
            print(f"\n‚ú® Mod√®le fine-tun√© ({result['fine_tuned_model']}):")
            print(f"   Tokens: {result['ft_tokens']} | Longueur: {result['ft_length']} caract√®res")
            if detailed:
                print(f"   R√©ponse: {result['ft_response']}")
            else:
                preview = result['ft_response'][:200] if result['ft_response'] else "N/A"
                print(f"   R√©ponse: {preview}{'...' if len(result['ft_response'] or '') > 200 else ''}")
        
        if not result["base_error"] and not result["ft_error"]:
            diff = result['length_diff']
            diff_pct = (diff / result['base_length'] * 100) if result['base_length'] > 0 else 0
            print(f"\nüìä Diff√©rence: {diff:+d} caract√®res ({diff_pct:+.1f}%)")
    
    # Statistiques globales
    print(f"\n{'='*80}")
    print("STATISTIQUES GLOBALES")
    print(f"{'='*80}")
    
    successful = [r for r in results if not r["base_error"] and not r["ft_error"]]
    if successful:
        avg_base_tokens = sum(r["base_tokens"] for r in successful) / len(successful)
        avg_ft_tokens = sum(r["ft_tokens"] for r in successful) / len(successful)
        avg_base_len = sum(r["base_length"] for r in successful) / len(successful)
        avg_ft_len = sum(r["ft_length"] for r in successful) / len(successful)
        
        print(f"Comparaisons r√©ussies: {len(successful)}/{len(results)}")
        print(f"Tokens moyens - Base: {avg_base_tokens:.1f} | Fine-tun√©: {avg_ft_tokens:.1f}")
        print(f"Longueur moyenne - Base: {avg_base_len:.1f} | Fine-tun√©: {avg_ft_len:.1f}")
        print(f"Diff√©rence moyenne: {avg_ft_len - avg_base_len:+.1f} caract√®res")


def save_results(results: List[Dict[str, Any]], output_file: str):
    """
    Sauvegarde les r√©sultats dans un fichier JSON.
    
    Args:
        results: Liste des r√©sultats de comparaison
        output_file: Chemin vers le fichier de sortie
    """
    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\n‚úì R√©sultats sauvegard√©s dans: {output_file}")


def load_prompts_from_file(file_path: str) -> List[str]:
    """
    Charge des prompts depuis un fichier JSONL (format instruction).
    
    Args:
        file_path: Chemin vers le fichier JSONL
        
    Returns:
        Liste des prompts (instructions)
    """
    prompts = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                data = json.loads(line)
                prompt = data.get("instruction", "")
                if prompt:
                    prompts.append(prompt)
    return prompts


def main():
    parser = argparse.ArgumentParser(
        description="Comparaison de mod√®les via l'API Mistral",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    
    # Mod√®les
    parser.add_argument("--base_model", required=True, help="Nom du mod√®le de base (ex: open-mistral-7b)")
    parser.add_argument("--fine_tuned_model", required=True, help="Nom du mod√®le fine-tun√© (ex: ft:open-mistral-7b:XXX)")
    
    # Prompts
    parser.add_argument("--prompts", nargs="+", help="Liste de prompts √† tester")
    parser.add_argument("--prompts_file", help="Fichier JSONL avec des prompts (format instruction)")
    
    # Options de g√©n√©ration
    parser.add_argument("--temperature", type=float, default=0.7, help="Temp√©rature (d√©faut: 0.7)")
    parser.add_argument("--max_tokens", type=int, default=512, help="Nombre max de tokens (d√©faut: 512)")
    
    # Sortie
    parser.add_argument("--output", help="Fichier JSON pour sauvegarder les r√©sultats")
    parser.add_argument("--detailed", action="store_true", help="Afficher les r√©ponses compl√®tes")
    
    args = parser.parse_args()
    
    # V√©rifier la cl√© API
    api_key = os.getenv("MISTRAL_API_KEY")
    if not api_key:
        raise ValueError(
            "MISTRAL_API_KEY non d√©finie. "
            "D√©finissez-la avec: export MISTRAL_API_KEY='votre-cl√©'"
        )
    
    # Charger les prompts
    if args.prompts:
        prompts = args.prompts
    elif args.prompts_file:
        prompts = load_prompts_from_file(args.prompts_file)
    else:
        # Prompts par d√©faut pour tester
        prompts = [
            "Qu'est-ce que le PTO ?",
            "Define KPI in one sentence.",
            "Explique le concept de burn rate en startup.",
        ]
        print("‚ö† Aucun prompt fourni, utilisation de prompts par d√©faut")
    
    if not prompts:
        raise ValueError("Aucun prompt √† tester")
    
    print(f"Comparaison de {len(prompts)} prompt(s)")
    print(f"  Mod√®le de base: {args.base_model}")
    print(f"  Mod√®le fine-tun√©: {args.fine_tuned_model}")
    
    # Initialiser le client
    client = Mistral(api_key=api_key)
    
    # Comparer les mod√®les
    results = compare_responses(
        client,
        args.base_model,
        args.fine_tuned_model,
        prompts,
        temperature=args.temperature,
        max_tokens=args.max_tokens,
    )
    
    # Afficher les r√©sultats
    print_comparison(results, detailed=args.detailed)
    
    # Sauvegarder si demand√©
    if args.output:
        save_results(results, args.output)


if __name__ == "__main__":
    main()

